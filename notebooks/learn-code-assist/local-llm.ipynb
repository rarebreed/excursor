{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a local LLM for code assist\n",
    "\n",
    "I struggled quite a bit to get the llama3 model working with the vscode continue plugin.  This document will describe  \n",
    "what to do in order to use the llama3 model with the vscode continue plugin.\n",
    "\n",
    "- Install ollama provider\n",
    "- Install VSCode Continue\n",
    "- Pull the llama3:8b model for chat\n",
    "- Pull the starcode2:3b model for code\n",
    "- ollama run llama3\n",
    "\n",
    "\n",
    "## Install ollama provider\n",
    "\n",
    "Go to [ollama.com](https://ollama.com/) and follow the directions for your OS\n",
    "\n",
    "## Install VSCode Continue\n",
    "\n",
    "The first step is to install the [vscode-continue](https://github.com/microsoft/vscode-continue) extension.  You can \n",
    "install it from the CLI using the following command:\n",
    "\n",
    "```\n",
    "code --install-extension Continue.continue\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!code --install-extension Continue.continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign agrreement on huggingfaces\n",
    "\n",
    "In order to use llama3, you need to sign an agreement with meta through huggingfaces.  Go to this page and follow the \n",
    "instuctions\n",
    "\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "\n",
    "Once you have agreed to their terms (which mostly only matters if you use thir model for inference with 700k concurrent\n",
    "users), then you will be able to use it for huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull the llama 3.1 for ollama\n",
    "\n",
    "Ollama will need the actual llama 3.1:3b model which we can like this once ollama is installed\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.1:8b\n",
    "ollama ls\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.1:8b\n",
    "!ollama ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run llama3.1 from ollama\n",
    "\n",
    "We need to actually have a running instance so that VS code continue plugin can interface with it.\n",
    "\n",
    "```bash\n",
    "ollama run llama3.1:8b\n",
    "```\n",
    "\n",
    "Open a new shell to run the above command since it will stay open.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
