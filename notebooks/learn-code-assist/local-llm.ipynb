{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a local LLM for code assist\n",
    "\n",
    "I struggled quite a bit to get the llama3 model working with the vscode continue plugin.  This document will describe\n",
    "what to do in order to use the llama3 model with the vscode continue plugin.\n",
    "\n",
    "- Install ollama provider\n",
    "- Install VSCode Continue\n",
    "- Pull the llama3:8b model for chat\n",
    "- Pull the starcode2:3b model for code\n",
    "- ollama run llama3\n",
    "\n",
    "\n",
    "## Install ollama provider\n",
    "\n",
    "Go to [ollama.com](https://ollama.com/) and follow the directions for your OS\n",
    "\n",
    "## Install VSCode Continue\n",
    "\n",
    "The first step is to install the [vscode-continue](https://github.com/microsoft/vscode-continue) extension.  You can \n",
    "install it from the CLI using the following command:\n",
    "\n",
    "```\n",
    "code --install-extension Continue.continue\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!code --install-extension Continue.continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
