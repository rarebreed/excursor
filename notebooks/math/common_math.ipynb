{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common math used in ML\n",
    "\n",
    "This guide will cover the most common non-linear and non calculus functions in Machine Learning, and how to express them\n",
    "in numpy and pytorch.  It will also cover why they are useful\n",
    "\n",
    "- Activation functions\n",
    "    - Softmax/Sigmoid\n",
    "    - ReLU\n",
    "- Entropy and Cross-Entropy\n",
    "- Seeding\n",
    "- Minmax\n",
    "- Mean and variance\n",
    "- Sampling\n",
    "- T-test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports we need\n",
    "\n",
    "import torch as tch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "An activation function in deep learning is used to turn a linear equation into a non-linear one.  Without activation\n",
    "functions, we would not be able to model things statistically as we do, and problems would boil down to a system of\n",
    "linear equations.\n",
    "\n",
    "The two most common activation functions are Sigmod (or softmax) and ReLU (Rectified Linear Unit)\n",
    "\n",
    "$\\sigma_i = \\frac{e^{z_i}}{\\Sigma e^z}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
