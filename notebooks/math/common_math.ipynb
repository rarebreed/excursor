{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common math used in ML\n",
    "\n",
    "This guide will cover the most common non-linear and non calculus functions in Machine Learning, and how to express them  \n",
    "in numpy and pytorch.  It will also cover why they are useful\n",
    "\n",
    "- Activation functions\n",
    "    - Softmax/Sigmoid\n",
    "    - ReLU\n",
    "- Entropy and Cross-Entropy\n",
    "- Seeding\n",
    "- Minmax\n",
    "- Mean and variance\n",
    "- Sampling\n",
    "- T-test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports we need\n",
    "\n",
    "import torch as tch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "An activation function in deep learning is used to turn a linear equation into a non-linear one.  Without activation  \n",
    "functions, we would not be able to model things statistically as we do, and problems would boil down to a system of  \n",
    "linear equations.\n",
    "\n",
    "The two most common activation functions are Sigmoid (or softmax) and ReLU (Rectified Linear Unit)\n",
    "\n",
    "$ \\huge{\\sigma_i = \\frac{e^{z_i}}{\\sum e^z}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z: tch.Tensor):\n",
    "    \"\"\"Generate non-linear mapping of input to output of probability\n",
    "\n",
    "    the z values are the input values, which get mapped to a number representing the probability.  This\n",
    "    is often used for classification.  The sum of the output values will always = 0\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : tch.Tensor\n",
    "        input values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tch.Tensor\n",
    "        mapped probability of ith values\n",
    "    \"\"\"\n",
    "    num = z.exp()  # e^z[i] for each element in z\n",
    "    denom = tch.sum(num)\n",
    "    return num / denom\n",
    "\n",
    "z = tch.rand(3)\n",
    "print(z)\n",
    "softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy and Cross Entropy\n",
    "\n",
    "Entropy is a measure of \"surprise\" or conversely, how much we don't know the probability of something.  50/50 odds are  \n",
    "the most \"surprising\" and the highest entropy, because we don't know what outcome is more likely.  When something has a  \n",
    "90% chance or 10% chance then the outcome (whether for or against) are better known, and thus have low entropy.  Another  \n",
    "way to think about entropy is that low entropy provides less information and high entropy provides more.\n",
    "\n",
    "Entropy is measured as:\n",
    "\n",
    "$ \\large{H = - \\sum_{i}^{N} p(x_i) \\log_{2}(p(x_i))} $\n",
    "\n",
    "Where \n",
    "- `p(x_i)` is the probability of the event happening\n",
    "\n",
    "The sum of p(x_i) should equal 1\n",
    "\n",
    "```python\n",
    "events = [.25, .75]\n",
    "assert sum(events) == 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(x: tch.Tensor):\n",
    "    return -1 * tch.sum(x * tch.log(x))\n",
    "\n",
    "x = tch.tensor([.25, .75])\n",
    "entropy(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "Cross entropy measures the difference between two random variables or sets of data.\n",
    "\n",
    "It is defined\n",
    "\n",
    "$ \\large{\\sigma^2 = \\frac{1}{n - 1} \\sum_{i=1}^{N} x_i -  \\log_{2}p(x_i)} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(t: tch.Tensor):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and Variance\n",
    "\n",
    "The mean tends to tell us what the most common value is in a set of data, but it can have problems depending on the distribution\n",
    "or variance of data.\n",
    "\n",
    "The mean or average is defined as:\n",
    "\n",
    "$ \\large{\\overline{x} = \\frac{1}{n} \\sum_{i}^{n} x_{i}} $\n",
    "\n",
    "```python\n",
    "nums = [2, 5, -1, 3]\n",
    "avg = sum(nums)/len(nums)\n",
    "```\n",
    "\n",
    "The variance of a data set is a measure of how dispersed the values are.  Imagine a curve that is not too high but broad, vs a curve\n",
    "that is tall but narrow, where both curves are centered on the same mean.  Variance is a way to measure how spread out or variable \n",
    "values are from the mean.\n",
    "\n",
    "Variance is defined as:\n",
    "\n",
    "$ \\large{\\sigma^2 = \\frac{1}{n - 1} \\sum_{i=1}^{N} (x_{i} - \\overline{x})^2} $\n",
    "\n",
    "The standard deviation is related to variance, and is the square rroot of the variance.\n",
    "\n",
    "$ \\large{\\sigma = \\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^{N} ((x_i) -  \\log_{2}(p(x_i))}} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
