{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common math used in ML\n",
    "\n",
    "This guide will cover the most common non-linear and non calculus functions in Machine Learning, and how to express them  \n",
    "in numpy and pytorch.  It will also cover why they are useful\n",
    "\n",
    "- Activation functions\n",
    "    - Softmax/Sigmoid\n",
    "    - ReLU\n",
    "- Entropy and Cross-Entropy\n",
    "- Seeding\n",
    "- Minmax\n",
    "- Mean and variance\n",
    "- Sampling\n",
    "- T-test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports we need\n",
    "\n",
    "import torch as tch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "An activation function in deep learning is used to turn a linear equation into a non-linear one.  Without activation  \n",
    "functions, we would not be able to model things statistically as we do, and problems would boil down to a system of  \n",
    "linear equations.\n",
    "\n",
    "The three most common activation functions are Sigmoid, Softmax and ReLU (Rectified Linear Unit)\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "This function, which always ranges between [0, 1], is useful for binary classification, but is used to make a function\n",
    "non-linear.  It is often found in hidden layers.  It is better to use ReLU for non-binary classification however. There\n",
    "are also some disadvantages with sigmoids, like vanishing gradients which in turn affect performance.  It is also more\n",
    "computationally expensive to perform.\n",
    "\n",
    "$ \\huge{\\sigma(x) = \\frac{1}{1 + e^{-x}}} $\n",
    "\n",
    "\n",
    "### Softmax\n",
    "\n",
    "The softmax turns a vector in $ \\mathbb{R} $ to a probability distribution.  It is often used in multiclass classification\n",
    "problems.  Therefore it is often the last activation used in the final output layer.\n",
    "\n",
    "$ \\huge{Softmax(z) = \\frac{e^{z_i}}{\\sum e^z}} $\n",
    "\n",
    "### Rectified Linear Unit (ReLU)\n",
    "\n",
    "It maps a value from [0, inf]. but has some advantages over sigmoids.  It is faster to compute and does not suffer the\n",
    "vanishing gradient problem.  It is commonly used in hidden layers.  It is 0 if x is negative, otherwise x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z: tch.Tensor):\n",
    "    \"\"\"Generate non-linear mapping of input to output of probability\n",
    "\n",
    "    the z values are the input values, which get mapped to a number representing the probability.  This\n",
    "    is often used for classification.  The sum of the output values will always = 0\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : tch.Tensor\n",
    "        input values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tch.Tensor\n",
    "        mapped probability of ith values\n",
    "    \"\"\"\n",
    "    num = z.exp()  # e^z[i] for each element in z\n",
    "    denom = tch.sum(num)\n",
    "    return num / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tch.rand(5)\n",
    "print(z)\n",
    "softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy and Cross Entropy\n",
    "\n",
    "Entropy is a measure of \"surprise\" or conversely, how much we don't know the probability of something.  50/50 odds are  \n",
    "the most \"surprising\" and the highest entropy, because we don't know what outcome is more likely.  When something has a  \n",
    "90% chance or 10% chance then the outcome (whether for or against) are better known, and thus have low entropy.  Another  \n",
    "way to think about entropy is that low entropy provides less information and high entropy provides more.\n",
    "\n",
    "Entropy is measured as:\n",
    "\n",
    "$ \\large{H = - \\sum_{i}^{N} p(x_i) \\log_{2}(p(x_i))} $\n",
    "\n",
    "Where \n",
    "- `p(x_i)` is the probability of the event happening\n",
    "\n",
    "The sum of p(x_i) should equal 1\n",
    "\n",
    "```python\n",
    "events = [.25, .75]\n",
    "assert sum(events) == 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(x: tch.Tensor):\n",
    "    return -1 * tch.sum(x * tch.log(x))\n",
    "\n",
    "x = tch.tensor([.25, .75])\n",
    "entropy(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "Cross entropy measures the difference between two random variables or sets of data.\n",
    "\n",
    "It is defined\n",
    "\n",
    "$ \\large{\\sigma^2 = \\frac{1}{n - 1} \\sum_{i=1}^{N} x_i -  \\log_{2}p(x_i)} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(t: tch.Tensor):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and Variance\n",
    "\n",
    "The mean tends to tell us what the most common value is in a set of data, but it can have problems depending on the  \n",
    "distribution or variance of data.\n",
    "\n",
    "The mean or average is defined as:\n",
    "\n",
    "$ \\large{\\overline{x} = \\frac{1}{n} \\sum_{i}^{n} x_{i}} $\n",
    "\n",
    "```python\n",
    "nums = [2, 5, -1, 3]\n",
    "avg = sum(nums)/len(nums)\n",
    "```\n",
    "\n",
    "The variance of a data set is a measure of how dispersed the values are.  Imagine a curve that is not too high but  \n",
    "broad, vs a curve that is tall but narrow, where both curves are centered on the same mean.  Variance is a way to  \n",
    "measure how spread out or variable values are from the mean.\n",
    "\n",
    "Variance is defined as:\n",
    "\n",
    "$ \\large{\\sigma^2 = \\frac{1}{n - 1} \\sum_{i=1}^{N} (x_{i} - \\overline{x})^2} $\n",
    "\n",
    "The standard deviation is related to variance, and is the square rroot of the variance.\n",
    "\n",
    "$ \\large{\\sigma = \\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^{N} ((x_i) -  \\log_{2}(p(x_i))}} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
