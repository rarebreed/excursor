{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common math used in ML\n",
    "\n",
    "This guide will cover the most common non-linear and non calculus functions in Machine Learning, and how to express them  \n",
    "in numpy and pytorch.  It will also cover why they are useful\n",
    "\n",
    "- Activation functions\n",
    "    - Softmax/Sigmoid\n",
    "    - ReLU\n",
    "- Entropy and Cross-Entropy\n",
    "- Seeding\n",
    "- Minmax\n",
    "- Mean and variance\n",
    "- Sampling\n",
    "- T-test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports we need\n",
    "\n",
    "import torch as tch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "An activation function in deep learning is used to turn a linear equation into a non-linear one.  Without activation  \n",
    "functions, we would not be able to model things statistically as we do, and problems would boil down to a system of  \n",
    "linear equations.\n",
    "\n",
    "The two most common activation functions are Sigmoid (or softmax) and ReLU (Rectified Linear Unit)\n",
    "\n",
    "$\\sigma_i = \\frac{e^{z_i}}{\\Sigma e^z}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z: tch.Tensor):\n",
    "    \"\"\"Generate non-linear mapping of input to output of probability\n",
    "\n",
    "    the z values are the input values, which get mapped to a number representing the probability.  This\n",
    "    is often used for classification.  The sum of the output values will always = 0\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : tch.Tensor\n",
    "        input values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tch.Tensor\n",
    "        mapped probability of ith values\n",
    "    \"\"\"\n",
    "    num = z.exp()  # e^z[i] for each element in z\n",
    "    denom = tch.sum(num)\n",
    "    return num / denom\n",
    "\n",
    "z = tch.rand(3)\n",
    "print(z)\n",
    "softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy and Cross Entropy\n",
    "\n",
    "Entropy is a measure of \"surprise\" or conversely, how much we don't know the probability of something.  50/50 odds are  \n",
    "the most \"surprising\" and the highest entropy, because we don't know what outcome is more likely.  When something has a  \n",
    "90% chance or 10% chance then the outcome (whether for or against) are better known, and thus have low entropy.  Another  \n",
    "way to think about entropy is that low entropy provides less information and high entropy provides more.\n",
    "\n",
    "Entropy is measured as:\n",
    "\n",
    "$ H = - \\sum_{i}^{N} p(x_i) \\log_{2}(p(x_i)) $\n",
    "\n",
    "Where \n",
    "- `x` are the data values\n",
    "- `p` is the probability\n",
    "\n",
    "The sum of p should equal 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(x: tch.Tensor):\n",
    "    return -1 * tch.sum(x * tch.log(x))\n",
    "\n",
    "x = tch.tensor([.25, .75])\n",
    "entropy(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
